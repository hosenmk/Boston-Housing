#import necessary libraries
import pandas as pd
import numpy as np
from sklearn import datasets
import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import roc_curve,RocCurveDisplay, plot_roc_curve, auc, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV
%matplotlib inline
import warnings
from sklearn import metrics
warnings.filterwarnings("ignore") # To ignore warning for boston dataset

import numpy as np
import pandas as pd
df=pd.read_csv('BostonHousing.csv')
x=41
X=.41

datarowsSeries=[pd.Series([0.069,10,2.3,0,0.53,6.5,65.2,4.01,1,290,15,395,4.9,24.0],index=df.columns),
pd.Series([0.69+x,10+x,2.3+X,0,0.5+X,6.5+X,65.2+X,4.1+X,1,290+x,15+x,395+x,4.9+X,24.3+x],index=df.columns),
pd.Series([0.68+x,11+x,2.4+X,0,0.6+X,6.6+X,65.1+X,4.0+X,1,291+x,13+x,390+x,4.2+X,24.2+x],index=df.columns),
pd.Series([0.67+x,12+x,2.5+X,0,0.4+X,6.5+X,65.3+X,4.2+X,1,292+x,14+x,392+x,4.3+X,24.1+x],index=df.columns),
pd.Series([0.66+x,13+x,2.4+X,0,0.7+X,6.5+X,65.4+X,4.1+X,1,293+x,16+x,391+x,4.4+X,24.2+x],index=df.columns)]
new_data=df.append(datarowsSeries,ignore_index=True)
new_data

new_data.shape
new_data.info()


import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
import seaborn as sns
Boxplot=new_data.drop('Price', axis = 'columns') #declared axis as columns
fig, axs = plt.subplots(1, len(Boxplot.columns), figsize=(20,10)) #Specifies the length of the figure
for i, Axis in enumerate(axs.flat):
  Axis.boxplot(Boxplot.iloc[:,i])
  Axis.set_title(Boxplot.columns[i], fontsize=12, fontweight='bold')
  Axis.tick_params(axis ='y', labelsize=18) 
plt.tight_layout()

new_data.describe()
new_data.mode()
new_data.median()
new_data.corr()

def function(x):
  if x<=20:
    return 'Low'
  else:
    return 'High'
new_data['Price']=new_data['Price'].apply(function)
sns.set_style('whitegrid');
sns.pairplot(new_data, hue = 'Price')
plt.show()

new_data['Price'].value_counts()

import numpy as np
objects = ('High', 'Low')
x_pos = np.arange(len(objects))
status_fre=[296, 215]
plt.bar(x_pos, status_fre)
plt.xticks(x_pos, objects)
plt.ylabel('No of house')
plt.title('House price')
plt.show()


import statsmodels.api as sm
x=new_data.drop('Price',axis='columns')
y=new_data['Price'].map({'Low':0,'High':1})
logitmodel=sm.Logit(y,x)
result=logitmodel.fit()
print(result.summary2())

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state= 113)


#####(i) Logistic Regression
from sklearn.linear_model import LogisticRegression
logmodel = LogisticRegression()
logmodel.fit(x_train, y_train)
y_pred=logmodel.predict(x_test)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score
print(classification_report(y_test, y_pred))
cm = confusion_matrix(y_test, y_pred)
print(confusion_matrix(y_test, y_pred))
print("Accuracy : ", accuracy_score(y_test, y_pred))
print("Precision : ", precision_score(y_test, y_pred))
specificity = cm[0,0]/(cm[0,0]+cm[0,1])
print('Specificity : ', specificity )
sensivity = cm[1,1]/(cm[1,0]+cm[1,1])
print('Sensivity : ', sensivity)

probs = logmodel.predict_proba(x_test)
predLR = probs[:,1]
from sklearn.metrics import roc_curve, roc_auc_score
auc = roc_auc_score(y_test, predLR)
print('LR AUC:', auc)

fpr, tpr, thresholds = roc_curve(y_test, predLR)
plt.figure()
lw = 2
plt.plot(fpr, tpr, color='Green',lw=lw, label='LR(AUC = %0.4f)' % auc)
plt.plot([0, 1], [0, 1], color='red', lw=lw, linestyle='-')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')

Text(0, 0.5, 'True Positive Rate')

## Decision tree
from sklearn import tree
DTclf=tree.DecisionTreeClassifier()
DTclf.fit(x_train, y_train)
y_pred= DTclf.predict(x_test)
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print(classification_report(y_test, y_pred))
cm = confusion_matrix(y_test, y_pred)
print(confusion_matrix(y_test, y_pred))
print("Accuracy : ", accuracy_score(y_test, y_pred))
print("Precision : ", precision_score(y_test, y_pred))
specificity = cm[0,0]/(cm[0,0]+cm[0,1])
print('Specificity : ', specificity )
sensivity = cm[1,1]/(cm[1,0]+cm[1,1])
print('Sensivity : ', sensivity)

probsDT = DTclf.predict_proba(x_test)
probsDT = probsDT[:, 1]
auc2 = roc_auc_score(y_test, probsDT)
print('DT AUC2:', auc2)

fpr2, tpr2, thresholds2 = roc_curve(y_test, probsDT)
plt.figure()
lw = 2

plt.plot([0, 1], [0, 1], color='red', lw=lw, linestyle='_')
plt.plot(fpr2, tpr2, color='green', lw=lw, label='DT(AUC = %0.f)' % auc2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')

###plt.title('Receiver operating characteristic')
plt.legend(loc='lower right')
plt.show()


###Random Forest
from sklearn.ensemble import RandomForestClassifier
RFclf=RandomForestClassifier(n_estimators=11)
RFclf.fit(x_train, y_train)
y_pred=RFclf.predict(x_test)
from sklearn.metrics import classificatio_report, confusion_matrix, accuracy_score
print(classification_report(y_test, y_pred))
cm = confusion_matrix(y_test, y_pred)
print(confusion_matrix(y_test, y_pred)
print("Accuracy : ", accuracy_score(y_test, y_pred))
print("Precision : ", precision_score(y_test, y_pred))
specificity = cm[0,0]/(cm[0,0]+cm[0,1])
print('Specificity : ', specificity)
sensivity = cm[1,1]/(cm[1,0]+cm[1,1])
print('Sensivity : ', sensivity)

probsRF = RFclf.predict_proba(x_test)
probsRF = probsRf[:, 1]
from sklearn.metrics import roc_curve, roc_auc_score
auc3 = roc_auc_score(y_test, probsRF)
print('RF AUC3:', auc3)

fpr3, tpr3, thresholds3 = roc_curve(y_test, probsRF)
plt.figure()
lw = 2

plt.plot([0, 1], [0, 1], color='red', lw=lw, linestyle='_')
plt.plot(fpr3, tpr3, color='green', lw=lw, label='RF(AUC = %0.4f)' % auc3)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
##plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.show()

### Support vector Machines
from sklearn.svm import SVC
SVclf = SVC(kernel='linear')
SVclf.fit(x_train, y_train)
y_pred=SVclf.predict(x_test)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print(classification_report(y_test, y_pred))
cm = confusion_matrix(y_test, y_pred)
print(confusion_matrix(y_test, y-pred))
print("Accuracy : ", accuracy_score(y_test, y_pred))
print("Precision : ", precision_score(y_test, y_pred))
specificity = cm[0,0]/(cm[0,0]+cm[0,1])
print("Specificity : ", specificity)
sensivity = cm[1,1]/(cm[1,0]+cm[1,1])
print("Sensivity : ", sensivity)




